{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will be using Credit card approval prediction dataset from Specific sources\n",
    "\n",
    "and the structure of the project will be follow:\n",
    "\n",
    "* To get a basic introduction of the project and whats the business problem associate with it?\n",
    "* start by loading and viewing the dataset.\n",
    "* To manipulate the data, if there are any missing values in the dataset.\n",
    "* To perform Exploratory data analysis(EDA) on the dataset\n",
    "* To preproccessing the data before applying ML model to our dataset.\n",
    "* Then, apply ML Model that can be predicted if an individual's application for credit card will be accepted or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit card applications and the problems associated with it\n",
    "\n",
    "Nowadays, banks receives a lot of applications for issuance of credit cards. Many of them rejected for many reasons, like high-loan balances, low income levels, or too many inquiries on an individual's credit report. Manually analysing these applications is error prone and time consuming process, for this we can be automated with the power of ML and pretty much every bank does so nowadays. In this project, we will be build a automated credit card aproval predictor using ML techniques, just like real banks do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Task\n",
    "\n",
    "Importing the pandas package and loading the dataset\n",
    "\n",
    "#####      1. Pandas : \n",
    "pandas is used to read the dataset file and import it as a dataframe, which is similar to a table with Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ar'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\\\Data Sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Married</th>\n",
       "      <th>BankCustomer</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>YearsEmployed</th>\n",
       "      <th>PriorDefault</th>\n",
       "      <th>Employed</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>DriversLicense</th>\n",
       "      <th>Citizen</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>income</th>\n",
       "      <th>ApprovalStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>280.0</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender    Age   Debt Married BankCustomer EducationLevel Ethnicity  \\\n",
       "0      b  30.83  0.000       u            g              w         v   \n",
       "1      a  58.67  4.460       u            g              q         h   \n",
       "2      a  24.50  0.500       u            g              q         h   \n",
       "3      b  27.83  1.540       u            g              w         v   \n",
       "4      b  20.17  5.625       u            g              w         v   \n",
       "\n",
       "   YearsEmployed PriorDefault Employed  CreditScore DriversLicense Citizen  \\\n",
       "0           1.25            t        t            1              f       g   \n",
       "1           3.04            t        t            6              f       g   \n",
       "2           1.50            t        f            0              f       g   \n",
       "3           3.75            t        t            5              t       g   \n",
       "4           1.71            t        f            0              f       s   \n",
       "\n",
       "   ZipCode  income ApprovalStatus  \n",
       "0    202.0       0              +  \n",
       "1     43.0     560              +  \n",
       "2    280.0     824              +  \n",
       "3    100.0       3              +  \n",
       "4    120.0       0              +  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset from client repository or DB\n",
    "# for that we have to use read_csv() method\n",
    "cols = [\"Gender\", \"Age\", \"Debt\", \"Married\", \"BankCustomer\", \"EducationLevel\", \"Ethnicity\", \"YearsEmployed\", \"PriorDefault\", \"Employed\", \"CreditScore\", \"DriversLicense\", \"Citizen\", \"ZipCode\", \"Income\", \"ApprovalStatus\"]\n",
    "df = pd.read_csv(\"credit-approval_csv.csv\")\n",
    "credit = df.rename(columns = {\"A1\":\"Gender\", \"A2\":\"Age\", \"A3\":\"Debt\", \"A4\":\"Married\", 'A5':\"BankCustomer\", 'A6':\"EducationLevel\", 'A7':\"Ethnicity\", 'A8':\"YearsEmployed\", 'A9':\"PriorDefault\", 'A10':\"Employed\", 'A11':\"CreditScore\", 'A12':\"DriversLicense\", 'A13':\"Citizen\", 'A14':\"ZipCode\", 'A15':\"income\", 'class':\"ApprovalStatus\"})\n",
    "credit.head()                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on above, the output appears a bit confusing at its first sight, but lets try figure out the most important features of credit card application. we find that since data is confidential, the contributor of this dataste has anonymized the feature names. The features of this dataset have been anonymized to protect the privacy, but it gives us a pretty good overview of the probably features.\n",
    "\n",
    "The probably features in a typical credit card applications are:\n",
    "\n",
    "* Gender\n",
    "* Age\n",
    "* Debt\n",
    "* Married\n",
    "* BankCustomer\n",
    "* EducationLevel\n",
    "* Ethnicity\n",
    "* YearsEmployed\n",
    "* PriorDefault\n",
    "* Employed\n",
    "* CreditScore\n",
    "* DriverseLicense\n",
    "* Citizen\n",
    "* ZipCode\n",
    "* Income\n",
    "* ApprovalStatus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Task\n",
    "\n",
    "The Dataset has a mixture of numerical and non-numerical features. It can be fixed with some preprocessing, but before we do that, lets learn about the dataset a bit more to see if there are other dataset issues that nedd to be fixed.\n",
    "\n",
    "So, lets start by printing the summary statistics using describe() method and dataframe information using info() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Age        Debt  YearsEmployed  CreditScore      ZipCode  \\\n",
      "count  678.000000  690.000000     690.000000    690.00000   677.000000   \n",
      "mean    31.568171    4.758725       2.223406      2.40000   184.014771   \n",
      "std     11.957862    4.978163       3.346513      4.86294   173.806768   \n",
      "min     13.750000    0.000000       0.000000      0.00000     0.000000   \n",
      "25%     22.602500    1.000000       0.165000      0.00000    75.000000   \n",
      "50%     28.460000    2.750000       1.000000      0.00000   160.000000   \n",
      "75%     38.230000    7.207500       2.625000      3.00000   276.000000   \n",
      "max     80.250000   28.000000      28.500000     67.00000  2000.000000   \n",
      "\n",
      "              income  \n",
      "count     690.000000  \n",
      "mean     1017.385507  \n",
      "std      5210.102598  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         5.000000  \n",
      "75%       395.500000  \n",
      "max    100000.000000  \n",
      "       Gender Married BankCustomer EducationLevel Ethnicity PriorDefault  \\\n",
      "count     678     684          684            681       681          690   \n",
      "unique      2       3            3             14         9            2   \n",
      "top         b       u            g              c         v            t   \n",
      "freq      468     519          519            137       399          361   \n",
      "\n",
      "       Employed DriversLicense Citizen ApprovalStatus  \n",
      "count       690            690     690            690  \n",
      "unique        2              2       3              2  \n",
      "top           f              f       g              -  \n",
      "freq        395            374     625            383  \n",
      "The dataset give the 6 features are numeric and remainings are non numeric features.\n",
      "The Describe() method is summary of statistics only for numeric features\n"
     ]
    }
   ],
   "source": [
    "##### To print summary statistics\n",
    "print(credit.describe())\n",
    "print(credit.describe(include = ['O']))\n",
    "\n",
    "print(\"The dataset give the 6 features are numeric and remainings are non numeric features.\\nThe Describe() method is summary of statistics only for numeric features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    374\n",
       "t    316\n",
       "Name: DriversLicense, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit[\"DriversLicense\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Gender          678 non-null    object \n",
      " 1   Age             678 non-null    float64\n",
      " 2   Debt            690 non-null    float64\n",
      " 3   Married         684 non-null    object \n",
      " 4   BankCustomer    684 non-null    object \n",
      " 5   EducationLevel  681 non-null    object \n",
      " 6   Ethnicity       681 non-null    object \n",
      " 7   YearsEmployed   690 non-null    float64\n",
      " 8   PriorDefault    690 non-null    object \n",
      " 9   Employed        690 non-null    object \n",
      " 10  CreditScore     690 non-null    int64  \n",
      " 11  DriversLicense  690 non-null    object \n",
      " 12  Citizen         690 non-null    object \n",
      " 13  ZipCode         677 non-null    float64\n",
      " 14  income          690 non-null    int64  \n",
      " 15  ApprovalStatus  690 non-null    object \n",
      "dtypes: float64(4), int64(2), object(10)\n",
      "memory usage: 86.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# To print dataset information\n",
    "credit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our dataset contains both numerical and non-numerical data(specifically data that are of \"float64\", \"int64\" and \"object\"), specifically that features are *Age*, *Debt*, *YearsEmployed*, *CreditScore*, *ZipCode* and *Income* features contains numerical values(float64, int64) and remaining all features contains non-numerical types(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Task\n",
    "##### Manipulating the data\n",
    "###### We have uncovered some issues that will effect the performance of our ML model if they go unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Married</th>\n",
       "      <th>BankCustomer</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>YearsEmployed</th>\n",
       "      <th>PriorDefault</th>\n",
       "      <th>Employed</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>DriversLicense</th>\n",
       "      <th>Citizen</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>income</th>\n",
       "      <th>ApprovalStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>b</td>\n",
       "      <td>25.83</td>\n",
       "      <td>12.835</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>cc</td>\n",
       "      <td>v</td>\n",
       "      <td>0.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>a</td>\n",
       "      <td>50.25</td>\n",
       "      <td>0.835</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>240.0</td>\n",
       "      <td>117</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.50</td>\n",
       "      <td>2.000</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>2.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>256.0</td>\n",
       "      <td>17</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>a</td>\n",
       "      <td>37.33</td>\n",
       "      <td>2.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>i</td>\n",
       "      <td>h</td>\n",
       "      <td>0.210</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>260.0</td>\n",
       "      <td>246</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>a</td>\n",
       "      <td>41.58</td>\n",
       "      <td>1.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.665</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>240.0</td>\n",
       "      <td>237</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>a</td>\n",
       "      <td>30.58</td>\n",
       "      <td>10.665</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>0.085</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>12</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>129.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>b</td>\n",
       "      <td>19.42</td>\n",
       "      <td>7.250</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>a</td>\n",
       "      <td>17.92</td>\n",
       "      <td>10.210</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>a</td>\n",
       "      <td>20.08</td>\n",
       "      <td>1.250</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>b</td>\n",
       "      <td>19.50</td>\n",
       "      <td>0.290</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0.290</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>280.0</td>\n",
       "      <td>364</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender    Age    Debt Married BankCustomer EducationLevel Ethnicity  \\\n",
       "671      b  25.83  12.835       u            g             cc         v   \n",
       "672      a  50.25   0.835       u            g             aa         v   \n",
       "673    NaN  29.50   2.000       y            p              e         h   \n",
       "674      a  37.33   2.500       u            g              i         h   \n",
       "675      a  41.58   1.040       u            g             aa         v   \n",
       "676      a  30.58  10.665       u            g              q         h   \n",
       "677      b  19.42   7.250       u            g              m         v   \n",
       "678      a  17.92  10.210       u            g             ff        ff   \n",
       "679      a  20.08   1.250       u            g              c         v   \n",
       "680      b  19.50   0.290       u            g              k         v   \n",
       "\n",
       "     YearsEmployed PriorDefault Employed  CreditScore DriversLicense Citizen  \\\n",
       "671          0.500            f        f            0              f       g   \n",
       "672          0.500            f        f            0              t       g   \n",
       "673          2.000            f        f            0              f       g   \n",
       "674          0.210            f        f            0              f       g   \n",
       "675          0.665            f        f            0              f       g   \n",
       "676          0.085            f        t           12              t       g   \n",
       "677          0.040            f        t            1              f       g   \n",
       "678          0.000            f        f            0              f       g   \n",
       "679          0.000            f        f            0              f       g   \n",
       "680          0.290            f        f            0              f       g   \n",
       "\n",
       "     ZipCode  income ApprovalStatus  \n",
       "671      0.0       2              -  \n",
       "672    240.0     117              -  \n",
       "673    256.0      17              -  \n",
       "674    260.0     246              -  \n",
       "675    240.0     237              -  \n",
       "676    129.0       3              -  \n",
       "677    100.0       1              -  \n",
       "678      0.0      50              -  \n",
       "679      0.0       0              -  \n",
       "680    280.0     364              -  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.iloc[671:681,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender            12\n",
       "Age               12\n",
       "Debt               0\n",
       "Married            6\n",
       "BankCustomer       6\n",
       "EducationLevel     9\n",
       "Ethnicity          9\n",
       "YearsEmployed      0\n",
       "PriorDefault       0\n",
       "Employed           0\n",
       "CreditScore        0\n",
       "DriversLicense     0\n",
       "Citizen            0\n",
       "ZipCode           13\n",
       "income             0\n",
       "ApprovalStatus     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has missing values, which we will takecare of in the manipulating process, the missing values in the dataset are labeled with *NaN*.\n",
    "we have to treat the missing values that we are going to perform manipulating process.\n",
    "\n",
    "An important question that get raised here is  `Why are we giving so much importance to missing values`? can't they be just ignored? ignoring missing values can effect the performance of a ML model heavily. While ignoring the missing values our model may miss out on information about the dataset that may be useful for its training. Then there are many models which can't handle missing values implicitly such as LDA(Linear Descriminant Analysis).\n",
    "\n",
    "So, to avoid this problem, we are going to impute the missing values with a stretegy called *Mean Imputation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values with Mean imputation\n",
    "credit.fillna(credit.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender            12\n",
       "Age                0\n",
       "Debt               0\n",
       "Married            6\n",
       "BankCustomer       6\n",
       "EducationLevel     9\n",
       "Ethnicity          9\n",
       "YearsEmployed      0\n",
       "PriorDefault       0\n",
       "Employed           0\n",
       "CreditScore        0\n",
       "DriversLicense     0\n",
       "Citizen            0\n",
       "ZipCode            0\n",
       "income             0\n",
       "ApprovalStatus     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully taken of the missing values present in the numeric columns. There are still some missing values to be imputed for columns like `Gender, Married, BankCustomer, EducationalLevel, Ethnicity`. All of these columns contains non-numerical data and this why the mean imputation stretagy would not work here. For this needs a different treatment.\n",
    "\n",
    "We are going to impute these missing values with the most frequent values as present in the repsective columns. For this we have to write the for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit[\"Gender\"].dtype == 'int64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit[\"Married\"].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit.fillna(credit[\"Gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender            0\n",
       "Age               0\n",
       "Debt              0\n",
       "Married           0\n",
       "BankCustomer      0\n",
       "EducationLevel    0\n",
       "Ethnicity         0\n",
       "YearsEmployed     0\n",
       "PriorDefault      0\n",
       "Employed          0\n",
       "CreditScore       0\n",
       "DriversLicense    0\n",
       "Citizen           0\n",
       "ZipCode           0\n",
       "income            0\n",
       "ApprovalStatus    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over each column of dataset\n",
    "for col in credit:\n",
    "    # Check if the column is object type\n",
    "    if credit[col].dtype == 'object':\n",
    "        # Impute the most frequent value\n",
    "        credit = credit.fillna(credit[col].value_counts().index[0])\n",
    "        \n",
    "credit.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above, impute the missing values in non-numerical features with most frequent values for each column\n",
    "\n",
    "* Missing values are successfully handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Task\n",
    "#### Preprocessing the data\n",
    "\n",
    "In the data, there are still minor but essential data preprocessing needed before we proceed towards building ML model.\n",
    "We are going to divide these remaining preprocessing steps into three main tasks:\n",
    "\n",
    "  1. Convert the non-numeric data into numeric.\n",
    "  2. Split the data into train and test sets.\n",
    "  3. Scale the feature values to a uniform range.\n",
    "\n",
    "First, we will converting all the non-numeric into numeric ones. We do this because not only it results in a faster computation but also many ML Models (like XGBoost) require the data to be in a strictly numeric format. We will do this by using a techinque called $LabelEncoding$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Gender          690 non-null    int32  \n",
      " 1   Age             690 non-null    float64\n",
      " 2   Debt            690 non-null    float64\n",
      " 3   Married         690 non-null    int32  \n",
      " 4   BankCustomer    690 non-null    int32  \n",
      " 5   EducationLevel  690 non-null    int32  \n",
      " 6   Ethnicity       690 non-null    int32  \n",
      " 7   YearsEmployed   690 non-null    float64\n",
      " 8   PriorDefault    690 non-null    int32  \n",
      " 9   Employed        690 non-null    int32  \n",
      " 10  CreditScore     690 non-null    int64  \n",
      " 11  DriversLicense  690 non-null    int32  \n",
      " 12  Citizen         690 non-null    int32  \n",
      " 13  ZipCode         690 non-null    float64\n",
      " 14  income          690 non-null    int64  \n",
      " 15  ApprovalStatus  690 non-null    int32  \n",
      "dtypes: float64(4), int32(10), int64(2)\n",
      "memory usage: 59.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# First, Import the label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Now, instantiated LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterate over all the values of each column and extract their dtypes\n",
    "for col in credit:\n",
    "    # compare if the dtype is 'object'\n",
    "    if credit[col].dtype == 'object':\n",
    "        # use labelEncoder to do the numeric_transformation\n",
    "        credit[col] = le.fit_transform(credit[col])\n",
    "        \n",
    "print(credit.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, some features are not important like `DriversLicense, ZipCode` features. So we should drop them to design our ML Model with the best set of features, This concept in *Data Science* is $Feature Selection$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop specific features using drop() method\n",
    "credit1 = credit.drop(['ZipCode','DriversLicense'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Age', 'Debt', 'Married', 'BankCustomer', 'EducationLevel',\n",
       "       'Ethnicity', 'YearsEmployed', 'PriorDefault', 'Employed', 'CreditScore',\n",
       "       'Citizen', 'income', 'ApprovalStatus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth Task\n",
    "#### Spliting the data\n",
    "\n",
    "Now, we split our dataset into train and test sets,to preprare our data for two different phases of ML modeling: Training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train_test_split() method from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(690, 13), (690,)]\n"
     ]
    }
   ],
   "source": [
    "# First, Convert dataFrame into Numpy array and\n",
    "# Segregate Features and labels into separate values\n",
    "X, y = credit1.iloc[:, 0:13].values, credit1.iloc[:, -1].values\n",
    "print([X.shape,y.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(517, 13), (173, 13), (517,), (173,)]\n"
     ]
    }
   ],
   "source": [
    "# Now, Split Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 1234)\n",
    "print([X_train.shape, X_test.shape, y_train.shape, y_test.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth task\n",
    "#### Feature Scalling\n",
    "\n",
    "The dataset also contains values from several ranges. Some features have a range of *0-28*, some features have a range of *2-67* and some have a range of *1017-100000*. Apart from these, we can get useful statistical information like *mean*, *Min* and *Max* about the features that have numerical values.\n",
    "\n",
    "Ideally, no information from the test data should be used to scale the training data or should be used to direct the training process of a machine learning model. \n",
    "Hence, we first split the data and then apply the scaling.\n",
    "Here, we have only one final preprocessing step before applying the ML model to data.\n",
    "\n",
    "Now, lets try to understand what the scaled values mean in the real world.\n",
    "Here, lets use `CreditScore` feature as example, the credit score of a person is their creditworthiness based on their credit history.\n",
    "Higher this number, the more financially trustworthy a person is consider to be.\n",
    "So, `CreditScore` of $1$ is the Highest since we are rescalling all the values to the range of $0-1$.\n",
    "\n",
    "For this we have to import $MinMaxScaler()$ method from *sklearn.preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.20932331 0.0297501  ... 0.         0.         0.00251   ]\n",
      " [1.         0.40977444 0.00158667 ... 0.04347826 1.         0.        ]\n",
      " [1.         0.09518797 0.0099167  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [1.         0.33954887 0.08429195 ... 0.04347826 0.         0.01187   ]\n",
      " [1.         0.31834586 0.099167   ... 0.         0.         0.002     ]\n",
      " [0.         0.03503759 0.01328838 ... 0.04347826 0.         0.00126   ]]\n"
     ]
    }
   ],
   "source": [
    "# Import MinMaxScaler() method\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Now Instantiate MinMaxScaler with mms_scaler variable and give \"feature_range\" parameter value is 0-1\n",
    "mms_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "# Use it to rescale X_train and X_test\n",
    "rescaled_X_train = mms_scaler.fit_transform(X_train)\n",
    "rescaled_X_test = mms_scaler.fit_transform(X_test)\n",
    "print(rescaled_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seventh Task\n",
    "#### Build the model to the training set\n",
    "\n",
    "Essentially, predicting if a credit card application will be approved or not is a classification task.\n",
    "In our dataset contains more instances that correpond to \"Denied\" status then instances corresponding to \"Approved\" status.\n",
    "Specifically, out of 100% instances, there are 55.5% applications that got \"Denied\" and 44.5% applications got \"Approved\"\n",
    "\n",
    "It gives us benchmark. A good ML model should be able to accuratly predict the status of the applications with respect to these statistics.\n",
    "\n",
    "Here, question rises like *Which model should we pick?* a question to ask is: *are the features that effect the credit card approval decision process correlated with each other?*. Although we can measure correlation, because of this correlation we will take advantage of the fact that generalized linear models perform well in these cases.\n",
    "\n",
    "\n",
    "##### sklearn package:\n",
    "  This ML library includes numerous ML algorithms already built-in with certain parameters set as default parameters so they work right out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Here, instantiate a Logisticregression classifier with defaul parameter values\n",
    "lgr = LogisticRegression()\n",
    "\n",
    "# Now, fit lgr model with train set\n",
    "lgr.fit(rescaled_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eight Task\n",
    "#### Making the predictions and evaluating the performance of the model\n",
    "\n",
    "Here, How well does our model perform?\n",
    "We will now evaluate our model on the test set with respect to classification accuracy.\n",
    "But, we will also take look the model's confusion matrix.\n",
    "In the case of predicting credit card applications, it is equally important to see if our ML model is able to predict the approval status of the application as \"Denied\" that originally got \"Denied\". If our model is not perform well in this aspect, then it might end up approving the application that should have been approved. The confusion matrix help us to view our model performance from these aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier is: 0.8439306358381503\n",
      "\n",
      "\n",
      "The Confusion Matrix of model is:\n",
      " [[67 11]\n",
      " [16 79]]\n"
     ]
    }
   ],
   "source": [
    "# Import Confusion_Matrix from sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use lgr model to predict instances from the rescaled test set and store it into y_pred variable\n",
    "y_pred = lgr.predict(rescaled_X_test)\n",
    "\n",
    "#print(y_pred)\n",
    "#print(y_test)\n",
    "# Now, get the accuracy score of lgr model and print it\n",
    "print(\"Accuracy of Logistic Regression classifier is:\", lgr.score(rescaled_X_test, y_test))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Now print the Confusion matrix of lgr model\n",
    "print(\"The Confusion Matrix of model is:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ninth Task\n",
    "#### GridSearch for model performance better\n",
    "\n",
    "The model is pretty good! It was able to yield an accuracy score of almost 84.4%.\n",
    "\n",
    "For the confusionMatrix, the first element of the first row of the confusion matrix denote the TrueNegative meaning the no.ofnegative instances (denied applications) predicted by the model correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
